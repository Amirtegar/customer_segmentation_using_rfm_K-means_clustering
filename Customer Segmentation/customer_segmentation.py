# -*- coding: utf-8 -*-
"""Capstone.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ngZlyHZqEd8XAFp0h77MqwqJDuf-E6Gg
"""

from google.colab import drive
drive.mount('/content/drive')

import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt

saas = pd.read_csv("/content/drive/MyDrive/TETRIS/SaaS-Sales.csv")
saas

saas.info()

saas.isna().sum()

saas[saas.duplicated()].shape[0]

saas.describe()

numerik = ['Sales', 'Quantity', 'Discount']
for col in numerik :
  sns.boxplot(data = saas, x = col )
  plt.show()

categorical_columns = ['Country', 'Region', 'Subregion', 'Industry', 'Segment', 'Product']
for column in categorical_columns:
    plt.figure(figsize=(8, 5))
    sns.countplot(x=column, data=saas, palette='viridis')
    plt.title(f'Countplot for {column}')
    plt.xticks(rotation=90)
    plt.show()

from datetime import datetime
saas['Order Date'] = pd.to_datetime(saas['Order Date'])
saas

saas_recency = saas.groupby(by='Customer',as_index=False)['Order Date'].max()
saas_recency.columns = ['CustomerName', 'LastPurchaseDate']
recent_date = saas_recency['LastPurchaseDate'].max()
saas_recency["Recency"] = saas_recency['LastPurchaseDate'].apply(lambda x: (recent_date - x).days)
saas_recency.head()

saas_frequency = saas.drop_duplicates().groupby(by=['Customer'], as_index=False)['Order Date'].count()
saas_frequency.columns = ['CustomerName', 'Frequency']
saas_frequency.head()

saas['Total'] = saas['Sales']*saas['Quantity']
saas_monetary = saas.groupby(by = 'Customer', as_index=False)['Total'].sum()
saas_monetary.columns =['CustomerName', 'Monetary',]
saas_monetary.head()



saas_rf = saas_recency.merge(saas_frequency, on='CustomerName')
saas_rfm = saas_rf.merge(saas_monetary, on='CustomerName').drop(columns='LastPurchaseDate')
saas_rfm.head()

saas_rfm1 = saas_rfm.drop(columns= 'CustomerName')
saas_rfm1

# from sklearn.preprocessing import StandardScaler
# standard_scaler = StandardScaler()
# standard_scaler.fit(saas_rfm1)
# rfm_normalized = standard_scaler.fit_transform(saas_rfm1)

from sklearn.preprocessing import RobustScaler
robust_scaler = RobustScaler()
rfm_normalized = robust_scaler.fit_transform(saas_rfm1)

rfm_normalized = pd.DataFrame(rfm_normalized)
rfm_normalized.columns = ['Recency', 'Frequency','Monetary']
rfm_normalized.head()

from sklearn.cluster import KMeans

distortions= []
K = range(1,15)
for k in K:
  kmeanmodel = KMeans(n_clusters=k)
  kmeanmodel.fit(rfm_normalized)
  distortions.append(kmeanmodel.inertia_)

plt.plot(K, distortions, 'bx-')
plt.xlabel('K')
plt.ylabel('Distortions')
plt.title('Elbow Showing For Optional K')
plt.show()

from sklearn.metrics import silhouette_score
sse_ = []
for k in range(2,15):
  kmeans = KMeans(n_clusters=k).fit(rfm_normalized)
  sse_.append([k, silhouette_score(rfm_normalized, kmeans.labels_)])

sse = pd.DataFrame(sse_)
sse.columns = ['clusters', 'sht_coeff']
print(sse)

sse_

plt.plot(pd.DataFrame(sse_)[0], pd.DataFrame(sse_)[1])
plt.title("Silhouette Coefficient")

kmeans = KMeans(n_clusters=3, max_iter=50)
kmeans.fit(rfm_normalized)

RFM = pd.concat([saas_rfm, pd.Series(kmeans.labels_)], axis=1)
RFM.columns = ['Costumer Name', 'Recency', 'Frequency', 'Monetary', 'Cluster']

RFM.describe()

RFM

model = KMeans(n_clusters=3, random_state=42)
model.fit(rfm_normalized)
model.labels_.shape

RFM.shape

RFM['Cluster'] = model.labels_
RFM.head()

RFM.groupby('Cluster').agg({'Recency':'mean', 'Frequency':'mean', 'Monetary':['mean', 'count']}).round()

RFM_normalized = pd.DataFrame(rfm_normalized, columns= ['Recency', 'Frequency', 'Monetary'])
RFM_normalized['ID']=RFM.index
RFM_normalized['Cluster'] = model.labels_
RFM_normalized.head()

RFM_melt = pd.melt(RFM_normalized.reset_index(), id_vars=['ID', 'Cluster'], value_vars=['Recency', 'Frequency', 'Monetary'], var_name='Attribute', value_name='Value')
RFM_melt.head()

fig = RFM_melt.groupby('Cluster').agg({'ID':lambda x:len(x)}).reset_index()

fig.rename(columns={'ID':'Count'}, inplace=True)
fig['Percent'] = (fig['Count']/fig['Count'].sum())*100
fig['Percent'] = fig['Percent'].round(1)
fig.head()

colors=['#bad0af','#d5e0cf','#f1f1f1','#f1d4d4'] #color palette
import plotly.express as px
fig_map = px.treemap(fig, path=['Cluster'], values='Count', width=800, height=400, title='Distribution of Cluster')
fig_map.update_layout(treemapcolorway=colors, margin=dict(t=25, l=25, r=25))
fig_map.data[0].textinfo = 'label+text+value+percent root'
fig_map.show()

sns.lineplot(x='Attribute', y='Value', hue='Cluster', data=RFM_melt)
plt.title('Line Plot of Attribute vs Value for each Cluster')
plt.xlabel('Attribute')
plt.ylabel('Value')
plt.show()

plt.pie(RFM_normalized.Cluster.value_counts(),
        labels=RFM_normalized.Cluster.value_counts().index,
        autopct='%.0f%%')
plt.show()